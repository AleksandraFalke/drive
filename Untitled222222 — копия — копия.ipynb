{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_1 = []\n",
    "pac = os.listdir('../../Test003/AAA1/' + \"/\" + \"new_images_train\")\n",
    "for i in range(len(pac)):\n",
    "    images = os.listdir('../../Test003/AAA1/new_images_train/' + \"/\" + pac[i])\n",
    "    for j in range(len(images)):\n",
    "        img = cv2.imread('../../Test003/AAA1/new_images_train/' + \"/\" + pac[i] + \"/\" + images[j])\n",
    "        images_1.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pacset_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frames</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[21, 31, 25], [21, 31, 25], [21, 31, 25], [2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[15, 16, 12], [15, 16, 12], [15, 16, 12], [1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[75, 89, 61], [75, 89, 61], [75, 89, 61], [7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[58, 75, 66], [57, 74, 65], [56, 73, 64], [5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[14, 15, 11], [14, 15, 11], [14, 15, 11], [1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              frames  label\n",
       "0  [[[21, 31, 25], [21, 31, 25], [21, 31, 25], [2...      1\n",
       "1  [[[15, 16, 12], [15, 16, 12], [15, 16, 12], [1...      1\n",
       "2  [[[75, 89, 61], [75, 89, 61], [75, 89, 61], [7...      1\n",
       "3  [[[58, 75, 66], [57, 74, 65], [56, 73, 64], [5...      1\n",
       "4  [[[14, 15, 11], [14, 15, 11], [14, 15, 11], [1...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['frames','label'])\n",
    "df['frames'] = images_1\n",
    "df['label'] = label\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images_1, label,  test_size=0.5,  random_state = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "#X_train = np.true_divide(X_train, 255.)\n",
    "#X_test = np.true_divide(X_test, 255.)\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "#X_train = np.true_divide(X_train, 255.)\n",
    "#X_test = np.true_divide(X_test, 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 480, 640, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       self.block_3 = torch.nn.Sequential(\n",
    "#                torch.nn.Conv1d(in_channels=120,\n",
    "#                                out_channels=16,\n",
    "#                                kernel_size=3,\n",
    "#                                stride=1,\n",
    "#                                padding=0),\n",
    "##                torch.nn.BatchNorm1d(28),\n",
    " #               torch.nn.PReLU(),\n",
    " #               torch.nn.Conv1d(in_channels=16,\n",
    " #                               out_channels=10,\n",
    " #                               kernel_size=1,\n",
    " #                               stride=1,\n",
    " #                               padding=1),\n",
    " #               torch.nn.BatchNorm1d(30)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 1])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class File(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(File, self).__init__()\n",
    "        \n",
    "        self.drop = torch.nn.Dropout(p=0.2)\n",
    "        self.prelu = torch.nn.PReLU()\n",
    "        self.conv = nn.Conv2d(480, 640, kernel_size=(4, 3), bias=False)\n",
    "        self.pool = nn.MaxPool2d(1, 2)\n",
    "        self.conv1 = nn.Conv2d(640, 160, kernel_size=(319, 1), bias=False)\n",
    "        self.pool1 = nn.MaxPool1d(1, 2) \n",
    "        self.conv2 = nn.Conv2d(160, 60, kernel_size=(1, 1), bias=False)\n",
    "        self.conv3 = nn.Conv1d(120, 120, 30, bias=False)\n",
    "\n",
    "        \n",
    " \n",
    "        self.block_1 = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(in_channels=120,\n",
    "                                out_channels=30,\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                padding=0),\n",
    "                torch.nn.BatchNorm1d(30),\n",
    "                torch.nn.PReLU(),\n",
    "                torch.nn.Conv1d(in_channels=30,\n",
    "                                out_channels=120,\n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm1d(30)\n",
    "        )\n",
    "        \n",
    "        self.block_2 = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(in_channels=120,\n",
    "                                out_channels=1,\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                padding=1),\n",
    "#                torch.nn.BatchNorm1d(3),\n",
    "                torch.nn.PReLU(),\n",
    "                torch.nn.Conv1d(in_channels=1,\n",
    "                                out_channels=120,\n",
    "                                kernel_size=3,\n",
    "                                stride=3,\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm1d(1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):           \n",
    "        x = self.prelu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.prelu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.prelu(self.conv2(x))\n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "        x = self.pool1(x)\n",
    "        x_1 = x\n",
    "        x = self.block_1(x)\n",
    "        x = self.prelu(x+x_1)\n",
    "        x = self.conv3(x)\n",
    "        x_1 = x\n",
    "        x = self.block_2(x)\n",
    "        x = self.prelu(x+x_1)\n",
    "        x = torch.round(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "neur = File()\n",
    "pred = neur(X_train)\n",
    "print(pred.shape)\n",
    "print(len(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loss(net, criterion, data_loader):\n",
    "    testing_loss = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = X_test.to(device), y_test.to(device)\n",
    "            outputs = neur(inputs)\n",
    "            outputs = outputs.view(outputs.size(0)* outputs.size(1))\n",
    "            labels = labels.view(labels.size(0)* labels.size(1))\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            testing_loss.append(loss.item())\n",
    "    return sum(testing_loss) / len(testing_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(neur.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASGD\n",
    "#Adam\n",
    "#NAdam\n",
    "#SGD\n",
    "#Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adadelta(neur.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "#criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ну что можно сказать,качесво обучения сильно завися от первоначального предсказания\n",
    "#Отобрала лучшие процессы обучения, однако мерика скачет как угорелая, она недостоверна,я думаю так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0] [it=    1] Train Loss: 0.998, Test Loss: 2.150 hamming Test:  0.5\n",
      "[ 1] [it=    2] Train Loss: 0.970, Test Loss: 2.126 hamming Test:  0.5\n",
      "[ 2] [it=    3] Train Loss: 0.935, Test Loss: 2.101 hamming Test:  0.5\n",
      "[ 3] [it=    4] Train Loss: 0.899, Test Loss: 2.077 hamming Test:  0.5\n",
      "[ 4] [it=    5] Train Loss: 0.867, Test Loss: 2.036 hamming Test:  0.5\n",
      "[ 5] [it=    6] Train Loss: 0.838, Test Loss: 2.012 hamming Test:  0.5\n",
      "[ 6] [it=    7] Train Loss: 0.803, Test Loss: 1.988 hamming Test:  0.5\n",
      "[ 7] [it=    8] Train Loss: 0.767, Test Loss: 1.963 hamming Test:  0.75\n",
      "[ 8] [it=    9] Train Loss: 0.735, Test Loss: 1.923 hamming Test:  0.75\n",
      "[ 9] [it=   10] Train Loss: 0.706, Test Loss: 1.898 hamming Test:  0.75\n",
      "[10] [it=   11] Train Loss: 0.670, Test Loss: 1.873 hamming Test:  0.75\n",
      "[11] [it=   12] Train Loss: 0.634, Test Loss: 1.849 hamming Test:  0.75\n",
      "[12] [it=   13] Train Loss: 0.601, Test Loss: 1.808 hamming Test:  0.75\n",
      "[13] [it=   14] Train Loss: 0.572, Test Loss: 1.783 hamming Test:  0.75\n",
      "[14] [it=   15] Train Loss: 0.535, Test Loss: 1.758 hamming Test:  0.75\n",
      "[15] [it=   16] Train Loss: 0.499, Test Loss: 1.733 hamming Test:  0.75\n",
      "[16] [it=   17] Train Loss: 0.467, Test Loss: 1.693 hamming Test:  0.75\n",
      "[17] [it=   18] Train Loss: 0.436, Test Loss: 1.668 hamming Test:  0.75\n",
      "[18] [it=   19] Train Loss: 0.400, Test Loss: 1.643 hamming Test:  0.75\n",
      "[19] [it=   20] Train Loss: 0.363, Test Loss: 1.618 hamming Test:  0.75\n",
      "[20] [it=   21] Train Loss: 0.331, Test Loss: 1.577 hamming Test:  0.75\n",
      "[21] [it=   22] Train Loss: 0.300, Test Loss: 1.552 hamming Test:  0.75\n",
      "[22] [it=   23] Train Loss: 0.263, Test Loss: 1.527 hamming Test:  0.75\n",
      "[23] [it=   24] Train Loss: 0.226, Test Loss: 1.501 hamming Test:  0.75\n",
      "[24] [it=   25] Train Loss: 0.194, Test Loss: 1.460 hamming Test:  0.75\n",
      "[25] [it=   26] Train Loss: 0.162, Test Loss: 1.435 hamming Test:  0.75\n",
      "[26] [it=   27] Train Loss: 0.124, Test Loss: 1.410 hamming Test:  1.0\n",
      "[27] [it=   28] Train Loss: 0.087, Test Loss: 1.384 hamming Test:  1.0\n",
      "[28] [it=   29] Train Loss: 0.056, Test Loss: 1.343 hamming Test:  1.0\n",
      "[29] [it=   30] Train Loss: 0.022, Test Loss: 1.318 hamming Test:  1.0\n",
      "[30] [it=   31] Train Loss: -0.016, Test Loss: 1.292 hamming Test:  1.0\n",
      "[31] [it=   32] Train Loss: -0.053, Test Loss: 1.266 hamming Test:  1.0\n",
      "[32] [it=   33] Train Loss: -0.138, Test Loss: 1.182 hamming Test:  1.0\n",
      "[33] [it=   34] Train Loss: -0.421, Test Loss: 1.118 hamming Test:  1.0\n",
      "[34] [it=   35] Train Loss: -0.801, Test Loss: 1.056 hamming Test:  1.0\n",
      "[35] [it=   36] Train Loss: -1.153, Test Loss: 0.971 hamming Test:  1.0\n",
      "[36] [it=   37] Train Loss: -1.442, Test Loss: 0.909 hamming Test:  1.0\n",
      "[37] [it=   38] Train Loss: -1.823, Test Loss: 0.848 hamming Test:  1.0\n",
      "[38] [it=   39] Train Loss: -2.149, Test Loss: 0.763 hamming Test:  0.75\n",
      "[39] [it=   40] Train Loss: -2.466, Test Loss: 0.703 hamming Test:  0.75\n",
      "[40] [it=   41] Train Loss: -2.847, Test Loss: 0.641 hamming Test:  0.75\n",
      "[41] [it=   42] Train Loss: -3.137, Test Loss: 0.558 hamming Test:  1.0\n",
      "[42] [it=   43] Train Loss: -3.493, Test Loss: 0.529 hamming Test:  1.0\n",
      "[43] [it=   44] Train Loss: -3.848, Test Loss: 0.482 hamming Test:  1.0\n",
      "[44] [it=   45] Train Loss: -4.036, Test Loss: 0.481 hamming Test:  1.0\n",
      "[45] [it=   46] Train Loss: -4.087, Test Loss: 0.485 hamming Test:  1.0\n",
      "[46] [it=   47] Train Loss: -4.139, Test Loss: 0.488 hamming Test:  1.0\n",
      "[47] [it=   48] Train Loss: -4.191, Test Loss: 0.491 hamming Test:  1.0\n",
      "[48] [it=   49] Train Loss: -4.243, Test Loss: 0.494 hamming Test:  1.0\n",
      "[49] [it=   50] Train Loss: -4.295, Test Loss: 0.498 hamming Test:  1.0\n"
     ]
    }
   ],
   "source": [
    "training_loss_d, testing_loss_d = [], []\n",
    "running_loss = []\n",
    "i = 0\n",
    "for epoch in range(50): \n",
    "    for data in train_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = X_train.to(device), y_train.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = neur(inputs)\n",
    "        outputs = outputs.view(outputs.size(0)* outputs.size(1))\n",
    "        labels = labels.view(labels.size(0)* labels.size(1))\n",
    "        #outputs = outputs[:-1]\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "#        outputs = outputs.round()\n",
    "#        labels = labels.round()\n",
    "#        outputs = outputs.detach().numpy()\n",
    "#        yy_train_t = yy_train_t.detach().numpy()\n",
    "#        score = accuracy_score(labels, outputs)\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "        i += 1\n",
    "        if i % 1 == 0:\n",
    "            avg_train_loss = sum(running_loss) / len(running_loss)\n",
    "            avg_test_loss, avg_test_score = get_test_loss(neur, criterion, valid_dataloader)\n",
    "            running_loss.clear()\n",
    "            training_loss_d.append(avg_train_loss)\n",
    "            testing_loss_d.append(avg_test_loss)\n",
    "            print(f\"[{epoch:2d}] [it={i:5d}] Train Loss: {avg_train_loss:.3f}, Test Loss: {avg_test_loss:.3f}\",'hamming Test: ', avg_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0] [epoch=    1] Train Loss: -0.623, Test Loss: -2.049\n",
      "[ 1] [epoch=    2] Train Loss: -0.650, Test Loss: -2.028\n",
      "[ 2] [epoch=    3] Train Loss: -0.679, Test Loss: -2.007\n",
      "[ 3] [epoch=    4] Train Loss: -0.708, Test Loss: -1.985\n",
      "[ 4] [epoch=    5] Train Loss: -0.737, Test Loss: -1.963\n",
      "[ 5] [epoch=    6] Train Loss: -0.767, Test Loss: -1.941\n",
      "[ 6] [epoch=    7] Train Loss: -0.797, Test Loss: -1.919\n",
      "[ 7] [epoch=    8] Train Loss: -0.827, Test Loss: -1.897\n",
      "[ 8] [epoch=    9] Train Loss: -0.857, Test Loss: -1.874\n",
      "[ 9] [epoch=   10] Train Loss: -0.888, Test Loss: -1.852\n",
      "[10] [epoch=   11] Train Loss: -0.919, Test Loss: -1.829\n",
      "[11] [epoch=   12] Train Loss: -0.950, Test Loss: -1.807\n",
      "[12] [epoch=   13] Train Loss: -0.981, Test Loss: -1.784\n",
      "[13] [epoch=   14] Train Loss: -1.012, Test Loss: -1.761\n",
      "[14] [epoch=   15] Train Loss: -1.044, Test Loss: -1.738\n",
      "[15] [epoch=   16] Train Loss: -1.076, Test Loss: -1.715\n",
      "[16] [epoch=   17] Train Loss: -1.108, Test Loss: -1.692\n",
      "[17] [epoch=   18] Train Loss: -1.140, Test Loss: -1.669\n",
      "[18] [epoch=   19] Train Loss: -1.173, Test Loss: -1.646\n",
      "[19] [epoch=   20] Train Loss: -1.205, Test Loss: -1.623\n",
      "[20] [epoch=   21] Train Loss: -1.238, Test Loss: -1.599\n",
      "[21] [epoch=   22] Train Loss: -1.271, Test Loss: -1.576\n",
      "[22] [epoch=   23] Train Loss: -1.304, Test Loss: -1.552\n",
      "[23] [epoch=   24] Train Loss: -1.337, Test Loss: -1.529\n",
      "[24] [epoch=   25] Train Loss: -1.371, Test Loss: -1.505\n",
      "[25] [epoch=   26] Train Loss: -1.405, Test Loss: -1.482\n",
      "[26] [epoch=   27] Train Loss: -1.438, Test Loss: -1.458\n",
      "[27] [epoch=   28] Train Loss: -1.473, Test Loss: -1.434\n",
      "[28] [epoch=   29] Train Loss: -1.507, Test Loss: -1.410\n",
      "[29] [epoch=   30] Train Loss: -1.541, Test Loss: -1.386\n",
      "[30] [epoch=   31] Train Loss: -1.576, Test Loss: -1.362\n",
      "[31] [epoch=   32] Train Loss: -1.611, Test Loss: -1.338\n",
      "[32] [epoch=   33] Train Loss: -1.646, Test Loss: -1.314\n",
      "[33] [epoch=   34] Train Loss: -1.681, Test Loss: -1.290\n",
      "[34] [epoch=   35] Train Loss: -1.717, Test Loss: -1.265\n",
      "[35] [epoch=   36] Train Loss: -1.753, Test Loss: -1.241\n",
      "[36] [epoch=   37] Train Loss: -1.789, Test Loss: -1.216\n",
      "[37] [epoch=   38] Train Loss: -1.825, Test Loss: -1.192\n",
      "[38] [epoch=   39] Train Loss: -1.861, Test Loss: -1.167\n",
      "[39] [epoch=   40] Train Loss: -1.898, Test Loss: -1.142\n",
      "[40] [epoch=   41] Train Loss: -1.935, Test Loss: -1.118\n",
      "[41] [epoch=   42] Train Loss: -1.972, Test Loss: -1.093\n",
      "[42] [epoch=   43] Train Loss: -2.009, Test Loss: -1.068\n",
      "[43] [epoch=   44] Train Loss: -2.047, Test Loss: -1.043\n",
      "[44] [epoch=   45] Train Loss: -2.084, Test Loss: -1.018\n",
      "[45] [epoch=   46] Train Loss: -2.122, Test Loss: -0.993\n",
      "[46] [epoch=   47] Train Loss: -2.161, Test Loss: -0.967\n",
      "[47] [epoch=   48] Train Loss: -2.199, Test Loss: -0.942\n",
      "[48] [epoch=   49] Train Loss: -2.238, Test Loss: -0.916\n",
      "[49] [epoch=   50] Train Loss: -2.277, Test Loss: -0.891\n"
     ]
    }
   ],
   "source": [
    "training_loss, testing_loss = [], []\n",
    "running_loss = []\n",
    "i = 0\n",
    "for epoch in range(50): \n",
    "    for data in train_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = X_train.to(device), y_train.to(device)\n",
    "        #inputs = torch.FloatTensor(inputs)\n",
    "        #labels = torch.FloatTensor(labels)\n",
    "        #inputs = inputs.type(torch.LongTensor)\n",
    "        #labels = labels.type(torch.LongTensor)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = neur(X_train)\n",
    "        #print(inputs)\n",
    "        outputs = outputs.view(outputs.size(0)* outputs.size(1))\n",
    "        labels = labels.view(labels.size(0)* labels.size(1)\n",
    "        #outputs = outputs[:-1]\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        #outputs = outputs.type(torch.LongTensor)\n",
    "        #print(len(outputs))\n",
    "        #print(len(labels))\n",
    "        loss = criterion(outputs, labels)\n",
    "        #print(outputs)\n",
    "        #print(labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "        i += 1\n",
    "        if i % 1 == 0:\n",
    "            avg_train_loss = sum(running_loss) / len(running_loss)\n",
    "            avg_test_loss = get_test_loss(neur, criterion, valid_dataloader)\n",
    "            running_loss.clear()\n",
    "            training_loss.append(avg_train_loss)\n",
    "            testing_loss.append(avg_test_loss)\n",
    "            print(f\"[{epoch:2d}] [epoch={i:5d}] Train Loss: {avg_train_loss:.3f}, Test Loss: {avg_test_loss:.3f}\")\n",
    "            #print('epoch: ', epoch,' Train Loss: ', avg_train_loss,' Test Loss: ', avg_test_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0] [epoch=    1] Train Loss: 13.275, Test Loss: 2.133\n",
      "[ 1] [epoch=    2] Train Loss: 13.207, Test Loss: 2.131\n",
      "[ 2] [epoch=    3] Train Loss: 13.137, Test Loss: 2.129\n",
      "[ 3] [epoch=    4] Train Loss: 13.066, Test Loss: 2.127\n",
      "[ 4] [epoch=    5] Train Loss: 12.994, Test Loss: 2.124\n",
      "[ 5] [epoch=    6] Train Loss: 12.922, Test Loss: 2.121\n",
      "[ 6] [epoch=    7] Train Loss: 12.849, Test Loss: 2.118\n",
      "[ 7] [epoch=    8] Train Loss: 12.775, Test Loss: 2.115\n",
      "[ 8] [epoch=    9] Train Loss: 12.701, Test Loss: 2.111\n",
      "[ 9] [epoch=   10] Train Loss: 12.627, Test Loss: 2.108\n",
      "[10] [epoch=   11] Train Loss: 12.553, Test Loss: 2.104\n",
      "[11] [epoch=   12] Train Loss: 12.478, Test Loss: 2.099\n",
      "[12] [epoch=   13] Train Loss: 12.403, Test Loss: 2.095\n",
      "[13] [epoch=   14] Train Loss: 12.327, Test Loss: 2.090\n",
      "[14] [epoch=   15] Train Loss: 12.252, Test Loss: 2.086\n",
      "[15] [epoch=   16] Train Loss: 12.176, Test Loss: 2.080\n",
      "[16] [epoch=   17] Train Loss: 12.100, Test Loss: 2.075\n",
      "[17] [epoch=   18] Train Loss: 12.024, Test Loss: 2.069\n",
      "[18] [epoch=   19] Train Loss: 11.947, Test Loss: 2.063\n",
      "[19] [epoch=   20] Train Loss: 11.871, Test Loss: 2.057\n",
      "[20] [epoch=   21] Train Loss: 11.794, Test Loss: 2.051\n",
      "[21] [epoch=   22] Train Loss: 11.717, Test Loss: 2.044\n",
      "[22] [epoch=   23] Train Loss: 11.640, Test Loss: 2.037\n",
      "[23] [epoch=   24] Train Loss: 11.562, Test Loss: 2.030\n",
      "[24] [epoch=   25] Train Loss: 11.485, Test Loss: 2.023\n",
      "[25] [epoch=   26] Train Loss: 11.407, Test Loss: 2.015\n",
      "[26] [epoch=   27] Train Loss: 11.330, Test Loss: 2.007\n",
      "[27] [epoch=   28] Train Loss: 11.252, Test Loss: 1.998\n",
      "[28] [epoch=   29] Train Loss: 11.174, Test Loss: 1.990\n",
      "[29] [epoch=   30] Train Loss: 11.095, Test Loss: 1.981\n",
      "[30] [epoch=   31] Train Loss: 11.017, Test Loss: 1.971\n",
      "[31] [epoch=   32] Train Loss: 10.939, Test Loss: 1.962\n",
      "[32] [epoch=   33] Train Loss: 10.860, Test Loss: 1.952\n",
      "[33] [epoch=   34] Train Loss: 10.781, Test Loss: 1.941\n",
      "[34] [epoch=   35] Train Loss: 10.702, Test Loss: 1.931\n",
      "[35] [epoch=   36] Train Loss: 10.623, Test Loss: 1.920\n",
      "[36] [epoch=   37] Train Loss: 10.544, Test Loss: 1.909\n",
      "[37] [epoch=   38] Train Loss: 10.465, Test Loss: 1.897\n",
      "[38] [epoch=   39] Train Loss: 10.385, Test Loss: 1.885\n",
      "[39] [epoch=   40] Train Loss: 10.306, Test Loss: 1.872\n",
      "[40] [epoch=   41] Train Loss: 10.226, Test Loss: 1.860\n",
      "[41] [epoch=   42] Train Loss: 10.146, Test Loss: 1.846\n",
      "[42] [epoch=   43] Train Loss: 10.066, Test Loss: 1.833\n",
      "[43] [epoch=   44] Train Loss: 9.986, Test Loss: 1.819\n",
      "[44] [epoch=   45] Train Loss: 9.905, Test Loss: 1.804\n",
      "[45] [epoch=   46] Train Loss: 9.825, Test Loss: 1.789\n",
      "[46] [epoch=   47] Train Loss: 9.744, Test Loss: 1.774\n",
      "[47] [epoch=   48] Train Loss: 9.664, Test Loss: 1.758\n",
      "[48] [epoch=   49] Train Loss: 9.583, Test Loss: 1.741\n",
      "[49] [epoch=   50] Train Loss: 9.502, Test Loss: 1.724\n"
     ]
    }
   ],
   "source": [
    "training_loss, testing_loss = [], []\n",
    "running_loss = []\n",
    "i = 0\n",
    "for epoch in range(50):\n",
    "    for data in train_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = X_train.to(device), y_train.to(device)\n",
    "        #inputs = torch.FloatTensor(inputs)\n",
    "        #labels = torch.FloatTensor(labels)\n",
    "        #inputs = inputs.type(torch.LongTensor)\n",
    "        #labels = labels.type(torch.LongTensor)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = neur(X_train)\n",
    "        #print(inputs)\n",
    "        outputs = outputs.view(outputs.size(0)* outputs.size(1))\n",
    "        labels = labels.view(labels.size(0)* labels.size(1))\n",
    "        #outputs = outputs[:-1]\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        #outputs = outputs.type(torch.LongTensor)\n",
    "        #print(len(outputs))\n",
    "        #print(len(labels))\n",
    "        loss = criterion(outputs, labels)\n",
    "        #print(outputs)\n",
    "        #print(labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "        i += 1\n",
    "        if i % 1 == 0:\n",
    "            avg_train_loss = sum(running_loss) / len(running_loss)\n",
    "            avg_test_loss = get_test_loss(neur, criterion, valid_dataloader)\n",
    "            running_loss.clear()\n",
    "            training_loss.append(avg_train_loss)\n",
    "            testing_loss.append(avg_test_loss)\n",
    "            print(f\"[{epoch:2d}] [epoch={i:5d}] Train Loss: {avg_train_loss:.3f}, Test Loss: {avg_test_loss:.3f}\")\n",
    "            #print('epoch: ', epoch,' Train Loss: ', avg_train_loss,' Test Loss: ', avg_test_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0] [epoch=    1] Train Loss: 1.033, Test Loss: 1.633\n",
      "[ 1] [epoch=    2] Train Loss: 1.008, Test Loss: 1.606\n",
      "[ 2] [epoch=    3] Train Loss: 0.982, Test Loss: 1.579\n",
      "[ 3] [epoch=    4] Train Loss: 0.956, Test Loss: 1.551\n",
      "[ 4] [epoch=    5] Train Loss: 0.930, Test Loss: 1.523\n",
      "[ 5] [epoch=    6] Train Loss: 0.904, Test Loss: 1.495\n",
      "[ 6] [epoch=    7] Train Loss: 0.877, Test Loss: 1.467\n",
      "[ 7] [epoch=    8] Train Loss: 0.850, Test Loss: 1.439\n",
      "[ 8] [epoch=    9] Train Loss: 0.823, Test Loss: 1.410\n",
      "[ 9] [epoch=   10] Train Loss: 0.796, Test Loss: 1.381\n",
      "[10] [epoch=   11] Train Loss: 0.768, Test Loss: 1.352\n",
      "[11] [epoch=   12] Train Loss: 0.741, Test Loss: 1.323\n",
      "[12] [epoch=   13] Train Loss: 0.713, Test Loss: 1.294\n",
      "[13] [epoch=   14] Train Loss: 0.685, Test Loss: 1.265\n",
      "[14] [epoch=   15] Train Loss: 0.657, Test Loss: 1.236\n",
      "[15] [epoch=   16] Train Loss: 0.629, Test Loss: 1.206\n",
      "[16] [epoch=   17] Train Loss: 0.601, Test Loss: 1.177\n",
      "[17] [epoch=   18] Train Loss: 0.573, Test Loss: 1.147\n",
      "[18] [epoch=   19] Train Loss: 0.545, Test Loss: 1.117\n",
      "[19] [epoch=   20] Train Loss: 0.516, Test Loss: 1.087\n",
      "[20] [epoch=   21] Train Loss: 0.488, Test Loss: 1.057\n",
      "[21] [epoch=   22] Train Loss: 0.459, Test Loss: 1.027\n",
      "[22] [epoch=   23] Train Loss: 0.430, Test Loss: 0.997\n",
      "[23] [epoch=   24] Train Loss: 0.401, Test Loss: 0.967\n",
      "[24] [epoch=   25] Train Loss: 0.372, Test Loss: 0.936\n",
      "[25] [epoch=   26] Train Loss: 0.343, Test Loss: 0.906\n",
      "[26] [epoch=   27] Train Loss: 0.314, Test Loss: 0.875\n",
      "[27] [epoch=   28] Train Loss: 0.285, Test Loss: 0.844\n",
      "[28] [epoch=   29] Train Loss: 0.255, Test Loss: 0.814\n",
      "[29] [epoch=   30] Train Loss: 0.226, Test Loss: 0.783\n",
      "[30] [epoch=   31] Train Loss: 0.196, Test Loss: 0.752\n",
      "[31] [epoch=   32] Train Loss: 0.166, Test Loss: 0.721\n",
      "[32] [epoch=   33] Train Loss: 0.136, Test Loss: 0.690\n",
      "[33] [epoch=   34] Train Loss: 0.107, Test Loss: 0.658\n",
      "[34] [epoch=   35] Train Loss: 0.076, Test Loss: 0.627\n",
      "[35] [epoch=   36] Train Loss: 0.046, Test Loss: 0.596\n",
      "[36] [epoch=   37] Train Loss: 0.016, Test Loss: 0.564\n",
      "[37] [epoch=   38] Train Loss: -0.014, Test Loss: 0.532\n",
      "[38] [epoch=   39] Train Loss: -0.045, Test Loss: 0.501\n",
      "[39] [epoch=   40] Train Loss: -0.076, Test Loss: 0.469\n",
      "[40] [epoch=   41] Train Loss: -0.106, Test Loss: 0.428\n",
      "[41] [epoch=   42] Train Loss: -0.134, Test Loss: 0.418\n",
      "[42] [epoch=   43] Train Loss: -0.160, Test Loss: 0.375\n",
      "[43] [epoch=   44] Train Loss: -0.186, Test Loss: 0.368\n",
      "[44] [epoch=   45] Train Loss: -0.214, Test Loss: 0.322\n",
      "[45] [epoch=   46] Train Loss: -0.239, Test Loss: 0.316\n",
      "[46] [epoch=   47] Train Loss: -0.268, Test Loss: 0.267\n",
      "[47] [epoch=   48] Train Loss: -0.292, Test Loss: 0.262\n",
      "[48] [epoch=   49] Train Loss: -0.322, Test Loss: 0.246\n",
      "[49] [epoch=   50] Train Loss: -0.346, Test Loss: 0.207\n"
     ]
    }
   ],
   "source": [
    "training_loss, testing_loss = [], []\n",
    "running_loss = []\n",
    "i = 0\n",
    "for epoch in range(50): \n",
    "    for data in train_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = X_train.to(device), y_train.to(device)\n",
    "        #inputs = torch.FloatTensor(inputs)\n",
    "        #labels = torch.FloatTensor(labels)\n",
    "        #inputs = inputs.type(torch.LongTensor)\n",
    "        #labels = labels.type(torch.LongTensor)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = neur(X_train)\n",
    "        #print(inputs)\n",
    "        outputs = outputs.view(outputs.size(0)* outputs.size(1))\n",
    "        labels = labels.view(labels.size(0)* labels.size(1))\n",
    "        #outputs = outputs[:-1]\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        #outputs = outputs.type(torch.LongTensor)\n",
    "        #print(len(outputs))\n",
    "        #print(len(labels))\n",
    "        loss = criterion(outputs, labels)\n",
    "        #print(outputs)\n",
    "        #print(labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "        i += 1\n",
    "        if i % 1 == 0:\n",
    "            avg_train_loss = sum(running_loss) / len(running_loss)\n",
    "            avg_test_loss = get_test_loss(neur, criterion, valid_dataloader)\n",
    "            running_loss.clear()\n",
    "            training_loss.append(avg_train_loss)\n",
    "            testing_loss.append(avg_test_loss)\n",
    "            print(f\"[{epoch:2d}] [epoch={i:5d}] Train Loss: {avg_train_loss:.3f}, Test Loss: {avg_test_loss:.3f}\")\n",
    "            #print('epoch: ', epoch,' Train Loss: ', avg_train_loss,' Test Loss: ', avg_test_loss )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
